{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport tensorflow as tf\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testC = test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train and test data set loaded \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can see the dataset has a lot of features(784 features ) and one target variable, so we want to reduce the dimension of the dataset, since there must be some features which does contain any information, so we will see how we can reduce the dimense from **n** to **k** where **k<n**"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train[\"label\"]\ntrain = train.drop(\"label\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nrows = 10\ncols=10\nfor i in range(rows*cols):\n    plt.subplot(rows,cols,i+1)\n    plt.imshow(train.iloc[i].values.reshape(28,28),cmap=\"afmhot\",interpolation=\"none\")\n    plt.xticks([])\n    plt.yticks([])\nplt.tight_layout()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we visualize the data how it looks like, what pictures it contains"},{"metadata":{},"cell_type":"markdown","source":"# **PCA**"},{"metadata":{},"cell_type":"markdown","source":"**Summary of Approach**\n1. standardize the data\n2. find covariance matrix of dataset\n3. find the eigen vectors and eigen values from covariance matrix(we can use correlation matrix also which is normalized version of covariance matrix)\n4. sort eigen values in descending order and choose k eigen vectors with highes eigen values or we can use scree plot for finding the number of k-value\n5. construct projection matrix which is transpose version of your selected K eigen vectors (shape of matrix should be after transpose **n_features X selected_PCs **)\n6. transform or project the original data on the projection matrix and you will get a matrix which has shape **n_samples X selected_PCs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ntrain_std = StandardScaler().fit_transform(train)\ntest_std = StandardScaler().fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we standardize the original data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mean = np.mean(train_std,axis=0)\ntrain_cov = np.dot(((train_std - train_mean).T),(train_std-train_mean))\neign_value,eign_vector = np.linalg.eig(train_cov)\n# eign_value_corr, eign_vector_corr = np.linalg.eig(train_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" we will calculate covarince matrix of dataset with the formula **np.dot((X-x_mean).T,(X,x_mean))** \n    and we will find the eigen stuff "},{"metadata":{"trusted":true},"cell_type":"code","source":"eign_pair = [(np.abs(eign_value[i]),eign_vector[:,i]) for i in range(len(eign_value))]\neign_pair.sort(key=lambda x:x[0],reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we sorted the eigen vectors corresponding to their eigen values from highest to lowest"},{"metadata":{"trusted":true},"cell_type":"code","source":"tot_eignV  = np.sum(eign_value)\neign_contr = [(sorted(eign_value,reverse=True)[i]/tot_eignV)*100 for i in range(len(eign_value))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we will see how much each eigen vector contain proportion of total variation of the data and we plot the results "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nplt.plot(eign_contr)\nplt.title(\"Importance of principal components using eigen value\") #higher the eigen_contr , higher importance\nplt.xlabel(\"principal components\")\nplt.ylabel(\"Variance contribution\")\n\nplt.subplot(1,2,2)\ndf = pd.DataFrame({\"variance_ratio\":eign_contr[:100],\"PCs\":[i for i in range(100)]})\nsns.barplot(y= 'variance_ratio',x=\"PCs\",data=df)\nplt.xticks([])\nplt.title(\"Scree plot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"First 50 Variables describe the variance proportion:%0.2f\"%np.sum(eign_contr[:50])+\"%\")\nprint(\"First 200 Variables describe the variance proportion:%0.2f\"%np.sum(eign_contr[:200])+\"%\")\nprint(\"First 400 Variables describe the variance proportion:%0.2f\"%np.sum(eign_contr[:400])+\"%\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see even after first 50 variables proprtion of each individual eigen vector contribute very less but as a whole they contribute more than 50% variance of the data therefor we should contain atleast 200 variables for the prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(45,25))\nfor i in range(50):\n    plt.subplot(5,10,i+1)\n    plt.imshow((eign_pair[i][1]*eign_pair[i][0]).reshape(28,28),cmap=\"afmhot\")\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(\"EigenValue\"+str(i+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are some plots of eigenVectors, as we can see eigenvector1 and eigenvector 50 are so much different in describing the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"w  = np.hstack([eign_pair[i][1].reshape(784,1) for i in range(10) ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we have made projection matrix as mentioned in the 5th step"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_projected = np.dot(train_std,w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"last step of PCA, we will project our original data onto the new feature space and we will plot first two principle component for visualizing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nscatter_data = go.Scatter(\n    x = X_projected[:6000][:,0],\n    y = X_projected[:6000][:,1],\n    mode = 'markers',\n    text = target[:6000],\n    showlegend = True,\n    marker = dict(\n        size = 8,\n        color = target[:6000],\n        colorscale ='Jet',\n        showscale = False,\n        line = dict(\n            width = 2,\n            color = 'rgb(255, 255, 255)'\n        ),\n        opacity = 0.8\n    )\n)\ndata = [scatter_data]\n\nlayout = go.Layout(\n    title= 'PCA',\n    hovermode= 'closest',\n    xaxis= dict(\n         title= 'PC1',\n        ticklen= 5,\n        zeroline= False,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'PC2',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig,filename=\"scatter-plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL TRAINING"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train/255.0\ntest = test/255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createPlaceholders(n_h,n_w,n_c,n_y):\n    X = tf.placeholder(np.float32,name=\"Xtrain\",shape=[None,n_h,n_w,n_c])\n    Y = tf.placeholder(np.float32,name=\"trueLabel\",shape=[None,n_y])\n    return X,Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters():\n\n    w1 = tf.get_variable(\"w1\", [3,3,1,8],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    w2 = tf.get_variable(\"w2\", [3,3,8,16],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    parameters = {\"W1\": w1,\n                  \"W2\": w2}\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_propagate(X,parameters):\n    w1 = parameters[\"W1\"]\n    w2 = parameters[\"W2\"]\n    layer1 = tf.nn.conv2d(X,w1,strides=[1,1,1,1],padding=\"SAME\")\n    layer1_activation = tf.nn.relu(layer1)\n    layer1_output = tf.nn.max_pool(layer1_activation,ksize=[1,4,4,1],strides=[1,4,4,1],padding=\"SAME\")\n    layer2 = tf.nn.conv2d(layer1_output,w2,strides=[1,1,1,1],padding=\"SAME\")\n    layer2_activation = tf.nn.relu(layer2)\n    layer2_output = tf.nn.max_pool(layer2_activation,ksize=[1,4,4,1],strides=[1,4,4,1],padding=\"SAME\")\n    fltn = tf.contrib.layers.flatten(layer2_output)\n    fully_con1 = tf.contrib.layers.fully_connected(fltn,100)\n    z3 = tf.contrib.layers.fully_connected(fully_con1,10,activation_fn=None)\n    return z3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(z3,y_true):\n    softmax_loss = tf.nn.softmax_cross_entropy_with_logits(logits=z3,labels=y_true)\n    cost  = tf.reduce_mean(softmax_loss)\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = np.reshape(test.values,(-1,28,28,1))\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrainx,testx,trainy,testy = train_test_split(train,target,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifiers  = []\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import ExtraTreesClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import BaggingClassifier\n# from sklearn.ensemble import AdaBoostClassifier\n# from sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed = 0\n# lr = LogisticRegression(random_state=seed,multi_class=\"multinomial\",solver=\"lbfgs\")\n# dt = DecisionTreeClassifier()\n# et  = ExtraTreesClassifier(verbose=1)\n# rf = RandomForestClassifier(verbose=1)\n# abc = AdaBoostClassifier()\n# gb = GradientBoostingClassifier(verbose=1)\n# classifiers.extend([lr,dt,et,rf,abc,gb])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import KFold, cross_val_score,cross_validate\n# def modelScore(model,X,y):\n#     cv = KFold(5,random_state=seed,shuffle=True).get_n_splits()\n#     score  = cross_validate(model,X,y,cv = cv,verbose=1)\n#     return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainScore = []\n# valScore=[]\n# std = []\n# for model in classifiers:\n#     score = modelScore(model,train,target)\n#     trainScore.append(score[\"train_score\"].mean())\n#     valScore.append(score[\"test_score\"].mean())\n#     std.append(score[\"test_score\"].std())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ntrainx = np.reshape(trainx.values,[-1,28,28,1])\ntestx = np.reshape(testx.values,[-1,28,28,1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainy  = to_categorical(trainy.values,10)\ntesty = to_categorical(testy.values,10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n\n    m = X.shape[0]                \n    mini_batches = []\n    np.random.seed(seed)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n\n    num_complete_minibatches = math.floor(m/mini_batch_size)\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n  \n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.framework import ops\nimport math\ndef model_evaluate(trainX,trainY,testX,testY,predictX,alpha= 0.01,mini_batch = 64,num_epochs=20,print_cost=True):\n    ops.reset_default_graph()                       \n    tf.set_random_seed(1)            \n    seed = 3     \n    (m, n_H0, n_W0, n_C0) = trainX.shape             \n    n_y = trainY.shape[1]                            \n    costs = []                \n    X, Y = createPlaceholders(n_H0,n_W0,n_C0,n_y)\n    parameters = initialize_parameters()\n    Z3 = forward_propagate(X, parameters)\n    cost = compute_cost(Z3,Y)\n  \n    optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(cost)\n  \n    init = tf.global_variables_initializer()\n \n    with tf.Session() as sess:\n        sess.run(init)\n        for epoch in range(num_epochs):\n            minibatch_cost = 0.\n            num_minibatches = int(m / mini_batch) \n            seed = seed + 1\n            minibatches = random_mini_batches(trainX, trainY, mini_batch, seed)\n            for minibatch in minibatches:\n                (minibatch_X, minibatch_Y) = minibatch\n                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\n                minibatch_cost += temp_cost \n            minibatch_cost = minibatch_cost/num_minibatches\n            if print_cost == True and epoch % 5 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n            if print_cost == True and epoch % 1 == 0:\n                costs.append(minibatch_cost)\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(alpha))\n        plt.show()\n        \n        predict_op = tf.argmax(Z3, 1)\n        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        print(accuracy)\n        train_accuracy = accuracy.eval({X: trainX, Y: trainY})\n        test_accuracy = accuracy.eval({X: testX, Y: testY})\n        print(\"Train Accuracy:\", train_accuracy)\n        print(\"Test Accuracy:\", test_accuracy)\n        test_prediction = predict_op.eval(feed_dict={X:predictX})\n        \n        return train_accuracy, test_accuracy, parameters,test_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, _, parameters,predictions= model_evaluate(trainx,trainy,testx,testy,test,alpha= 0.001,mini_batch = 64,num_epochs=85,print_cost=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nrows = 10\ncols=10\nfor i in range(rows*cols):\n    plt.subplot(rows,cols,i+1)\n    plt.imshow(testC.iloc[i].values.reshape(28,28),cmap=\"afmhot\",interpolation=\"none\")\n    plt.xticks([])\n    plt.yticks([])\nplt.tight_layout()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"ImageId\":[i+1 for i in range(len(predictions))],\"Label\":predictions})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}